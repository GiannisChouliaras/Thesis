{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d929775",
   "metadata": {},
   "source": [
    "# Global Averaging\n",
    "\n",
    "in this notebook, intermediate layers from vgg16 will be extracted. the output will be a feature vector (ex: 512, 18,18). 512 is the kernels and 18 the width and height. For every pixel, the average of those 512 values will be computed. \n",
    "\n",
    "The result is a vector (512,) where every kernel will have only one value. Using the cosine similarity and the euclidean distance, it computes the flatten array of the images before and after. Results will be stored in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb748cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Callable, Union\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca0c67",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Load the model vgg16, register the hook in a layer (ex: 30).\n",
    "Open and transform the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d222b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 11\n",
    "path = \"../data/preprocess/\"\n",
    "features = {}\n",
    "model_name = 'vgg16'\n",
    "\n",
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "def reg_hook(layer: int) -> Callable:\n",
    "    def hook(model, input, output):\n",
    "        features['feats'] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.features[layer].register_forward_hook(reg_hook('feats'))\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "def open_images(path: str, case: str, situation: str) -> list:\n",
    "    lst = []\n",
    "    for filename in glob.glob(path + case + \"/\" + situation + \"/*.JPG\"):\n",
    "        lst.append(Image.open(filename).convert(\"RGB\"))\n",
    "    return lst\n",
    "\n",
    "def extract_features(model: torchvision.models, transform: Callable, images: list) -> list:\n",
    "\n",
    "    # inner function to normalize the vector\n",
    "    def normalize(A: np.ndarray) -> np.ndarray:\n",
    "        norm = np.linalg.norm(A)\n",
    "        return A / norm\n",
    "\n",
    "    embeddings = []\n",
    "    \n",
    "    # iterate through all images and save the features in the list\n",
    "    for image in images:\n",
    "        x = transform(image).unsqueeze(0).to(\"cpu\")\n",
    "        _ = model(x)\n",
    "        embeddings.append(normalize(features['feats'].cpu().numpy()))\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c740244",
   "metadata": {},
   "source": [
    "## Distances\n",
    "\n",
    "for this notebook, cosine similarity and euclidean distance will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358fdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A: Union[np.ndarray, list], B: Union[np.ndarray, list]) -> tuple:\n",
    "    return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "\n",
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "def distance_metric(embeddings: list) -> float:\n",
    "    \n",
    "    A = embeddings[0]\n",
    "    B = embeddings[1]\n",
    "    C = embeddings[2]\n",
    "    \n",
    "    def transform_dist_to_sim(x: float) -> float:\n",
    "        return 1 / (1 + x)\n",
    "\n",
    "    e1 = transform_dist_to_sim(distance.euclidean(A, B))\n",
    "    e2 = transform_dist_to_sim(distance.euclidean(A, C))\n",
    "\n",
    "#     c1 = cosine_similarity(A, B)\n",
    "#     c2 = cosine_similarity(A, C)\n",
    "    \n",
    "\n",
    "    return Average([e1, e2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06d818b",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Opening the image and extract the feature\n",
    "\n",
    "#### Perform the global averaging\n",
    "With the feature vectors from the images, uses the function perform_global_averaging to get the mean of the pixels of every kernel.\n",
    "\n",
    "##### Result\n",
    "the result is a 1D array with the size of the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd38780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_averaging_on_kernel(vector: np.ndarray) -> np.ndarray:\n",
    "    for i, val in enumerate(vector):\n",
    "        vector[i] = np.mean(val)\n",
    "    return vector[:, 0, 0]\n",
    "\n",
    "def global_averaging_on_pixels(vector: np.ndarray) -> list:\n",
    "    vector = np.transpose(vector)\n",
    "    res = []\n",
    "    for i in range(vector.shape[0]):\n",
    "        for j in range(vector.shape[1]):\n",
    "            res.append(np.mean(vector[i][j]))\n",
    "    return res\n",
    "            \n",
    "\n",
    "cases = [f.path[19:] for f in os.scandir(path) if f.is_dir()]\n",
    "results = []\n",
    "\n",
    "for case in cases:\n",
    "    \n",
    "    # get the current case\n",
    "    images_before_treatment = open_images(path=path, case=case, situation='BEFORE')\n",
    "    images_after_treatment = open_images(path=path, case=case, situation='AFTER')\n",
    "\n",
    "    # get the embeddings\n",
    "    embeddings_before = extract_features(model=model, transform=transform, images=images_before_treatment)\n",
    "    embeddings_after = extract_features(model=model, transform=transform, images=images_after_treatment)\n",
    "    \n",
    "    # perform the global averaging on the embeddings\n",
    "    before_lst = [global_averaging_on_kernel(vector[0]) for vector in embeddings_before]\n",
    "    after_lst = [global_averaging_on_kernel(vector[0]) for vector in embeddings_after]\n",
    "    \n",
    "    # calculate distances\n",
    "    before_distances = distance_metric(before_lst)\n",
    "    after_distances = distance_metric(after_lst)\n",
    "    \n",
    "    # add the results in the final list\n",
    "    results.append((case, round(before_distances, 3), round(after_distances, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2d6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe and store it in a csv file.\n",
    "df = pd.DataFrame(results, columns=['Case', 'Before', 'After'])\n",
    "\n",
    "# store df in a csv file.\n",
    "df.to_csv(f'../csv/global_averaging/{model_name}_{layer}_global_avg_kernel_eucl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e00cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(before_lst[0].shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05842187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
